{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Transfer Learning to detect COVID-19\n",
    "\n",
    "Can we use Deep Learning to analyze a set of chest XRays of known COVID-19 patients to detect when a patient has COVID-19?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work and information in this notebook is inspired by a [blog post](https://www.pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/) by Adrian Rosebrock on his website [PyImageSearch](www.pyimagesearch.com)\n",
    "\n",
    "In that blog post, Adrian shows how to apply transfer learning using the [VGG16 Convolution Neural Network](https://neurohive.io/en/popular-networks/vgg16/) using the network weights from the ImageNet dataset.  \n",
    "\n",
    "My contribution to that great blog post is to go into some detail on what transfer learning is and why it was so powerful in this application.  I will then show how we can use other Deep Learning Convolutional networks for transfer learning and their performance on the same dataset.\n",
    "\n",
    "Lastly we will look at how a model trained on the COVID-19 XRay images does against images of normal chest XRays and chest XRays of those with Pneumonia to see if the model can accurately detect that chest XRay is NOT COVID-19.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"5\">Like Adrian, I want to be very clear that the information here, and derived from [Adrian's](https://www.pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/) blog post is meant for Deep Learning educational purposes only.  In no way is it implied that this technique should be used for the identification of COVID-19.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two datasets will be used for this analysis.  One one source collection of chest X-Rays of COVID-19 patients hosted on Github.  There other is from the Kaggle site which contains chest X-Rays of normal lung and those with pneumonia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19 Image Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a Github repo started by Joseph Paul Cohen called [covid-chestxray-dataset](https://github.com/ieee8023/covid-chestxray-dataset).  Joesph Paul Cohen has created a GitHub repo to collect chest X-Rays of anonymized COVID-9 patients. Â He is also collecting other respiratory X-Rays such as MERs and SARs.\n",
    "\n",
    "![Covid19Share](./notebook_images/covid19-share-image.png)\n",
    "\n",
    "From the README.md file in that Github repo:\n",
    "\n",
    "`We are building a database of COVID-19 cases with chest X-ray or CT images. We are looking for COVID-19 cases as well as MERS, SARS, and ARDS.`\n",
    "\n",
    "We will use this dataset to have a model learn what a chest XRay looks like that has COVID-19.\n",
    "\n",
    "This dataset is updated frequently.  New images were added as I was working with the dataset so check back often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Chest X-Ray Images (Pneumonia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle](www.kaggle.com) is an online community of people interested in data science.  It allows users to find, publish, explore and build models around datasets made available to the public.  \n",
    "\n",
    "The dataset we will use is the [Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) dataset.  This dataset has chest X-Ray images of normal lungs as well as chest X-Ray images of lungs with Pneumonia.\n",
    "\n",
    "We can use this dataset to train the model on what normal chest X-Rays look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspect of Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<table>\n",
    "  <tr style=\"text-align:center; background-color:white\">\n",
    "    <td> <img src=\"./notebook_images/covid19-collage.png\" alt=\"COVID\" style=\"width: 400px;\"/></td>\n",
    "    <td> <img src=\"./notebook_images/normal-collage.png\" alt=\"NORMAL\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"./notebook_images/pneumonia-collage.png\" alt=\"PNEU\" style=\"width: 400px;\"/> </td>\n",
    "  </tr>\n",
    "  <tr style=\"width:100%;\">\n",
    "    <td style=\"width:20%;\"> <p style=\"font-family:overpass;font-size:16px;text-align:center;color:#303030;font-weight:300;\">COVID-19</p> </td>\n",
    "    <td style=\"width:20%;\"> <p style=\"font-family:overpass;font-size:16px;text-align:center;color:#303030;font-weight:300;\">NORMAL</p> </td>\n",
    "    <td style=\"width:20%;\"> <p style=\"font-family:overpass;font-size:16px;text-align:center;color:#303030;font-weight:300;\">PNEUMONIA</p> </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to stress that the datasets being used are not vetted by myself or as far as I know, anyone with expertise in the field.  We are using datasets from disparate sources, collected at different times with different procedures.  I have no way of knowing if the image is really of a COVID-19 Chest X-Ray, or some other ailament that resembles COVID-19.\n",
    "\n",
    "So take this exercise as an interesting use case of applying Deep Transfer Learning to a set of images for classification.  \n",
    "\n",
    "The other aspect to consider is that there are not many examples of chest X-Rays of COVID-19 patients.  The lack of data will impact the degree of trust we can have in the results.  The more data we can collect, the better the training and high degree of confidence we will have in the models.  Until then, we can only work with what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Transfer Learning - Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of transfer learning; feature extraction and fine tuning.\n",
    "\n",
    "Fine tuning is when the fully connected network (FCN) layer of a convolutional neural network (CNNs) is removed and retrained with a new FCN layer.  But first - lets talk about CNNs.\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are commonly used when working with images and Deep Learning.\n",
    "\n",
    "Think of a convolution as a small, 3x3 or 5x5, etc, mathematical matrix that is applied to an image to alter the image.  We use convolutions all time when using image processing software to sharpen or blur an image.  The goal of training a CNN is to determine the values of the matrix convolutions that will alter the image in such a way as to expose features of the image that the FCN layer can use for classification.\n",
    "\n",
    "A CNN model will be made up of some number of convolutional layers.  Each layer will have a different number of kernels ( small matrix we talked about ), and a final fully connected network (FCN) layer that will be used to perform the actual classification step.\n",
    "\n",
    "The initial convolutional layers, also called the convolutional base, act as a feature extraction layer.  This layer is attempting to identify the features in a dataset and for an image that might be interesting parts of the images. According to Francois Chollet the creator of Keras from his book Deep Learning with Python,\n",
    "\n",
    "`... the representations learned by the convolutional base as likely to be more generic [than the fully connected layer] and therefore more reusable; the feature maps of the convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the conputer-vision problem at hand.`\n",
    "\n",
    "This means that the convolution layers can be trained to identify interesting features based on how the model was trained.  This does imply that the model was trained on images with some commonality to the new problem.\n",
    "\n",
    "`So if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using on the first few layers of the model to do feature extraction, rather than using the entire convolutional base`.\n",
    "\n",
    "The representation learned by the fully connected network layer will be specific to the new dataset that the model is trained on, as it will only contain information about the set of possible outcomes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![CNN Arch](notebook_images/A-convolutional-neural-networks-CNN.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<span><font size=\"2\">\n",
    "    A New Method for Face Recognition Using Convolutional Neural Network - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/A-convolutional-neural-networks-CNN_fig6_321286547 [accessed 18 Mar, 2020]\n",
    "    </font></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we can use a trained model leveraging all of the time and data it took for training the convolutional part, and just remove the FCN layer.  \n",
    "\n",
    "A properly trained CNN requires a lot of data and CPU/GPU time.  If our dataset is small, we cannot start from a fresh CNN model and try to train it and hope for good results. \n",
    "\n",
    "![cnnlayer](notebook_images/ccn_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leveraging the frozen CNN layer, we just have to train a new fully connected layer.  This requires much less CPU and while we would like as much data as is possible, we can obtain very good results with less data on the fully connected layer.\n",
    "\n",
    "![cnnlayer](notebook_images/cnn_new_fcn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Deep Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Tensorflow and Keras to build out a model for the COVID-19 Chest X-Ray detection.  \n",
    "\n",
    "Keras comes with a number of models with their pre-configured architecture.  You can find out more about the Keras models [here](https://keras.io/applications/).  Pre-configured means that the exact architecture in terms of the number of layers and number of kernels is already configured.  But these models have no weights.  It is the training process that determines the weights to use, however, we can use the weights that have already be determined to work well using large amounts of image data.  For that we will use the imagenet weights for each of the models.\n",
    "\n",
    "[ImageNet](http://imagenet.stanford.edu) is an image database of millions of images.  These images were used to train the model architectures and those best model weights can be made available so that we do not have to train a model from nothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to look at 4 Keras models:\n",
    "\n",
    "* VGG16 This was also the model that Adrian used in his [blog post](https://www.pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/).  More information can be found [here](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "* VGG19  More information can be found [here](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "* ResNet50  More information can be found [here](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "* ResNet50V2  More information can be found [here](https://arxiv.org/abs/1603.05027)\n",
    "\n",
    "The exact details of the models can be found from the links above\n",
    "\n",
    "We are going to perform the following on each of the 4 models:\n",
    "\n",
    "* Train the model on COVID-19 images and the same number of NORMAL Chest X-Ray images from the Kaggle dataset\n",
    "\n",
    "* Display the Confusion Matrix\n",
    "\n",
    "* Display the Accuracy\n",
    "\n",
    "* Sensitivity aka, Recall or True Positive Rate.  This is calculated as: TP/(TP + FN)\n",
    "\n",
    "Sensitivity is the models ability to predit true positives of each category. \n",
    "\n",
    "\n",
    "Sensitivity states, when we predict a chest X-Ray is a COVID-19 patient, how often did we get that right.  For example, if this number is 93%, this mean 93% of the time we correctly predicted the X-Ray as COVID-19.  This also means that 7% of the time we falsely said it was COVID-19, but really was NOT.  For this scenario, we would rather error on the side that the person had COVID-19 and quarentine even if it turns out they did not.\n",
    "\n",
    "\n",
    "* Specificity aka, True Negative Rate.  This is calculated as: TN/(TN+FP)\n",
    "\n",
    "\n",
    "Specificity is the metric that evaluates the models ability to predict true negatives of each category.  \n",
    "\n",
    "\n",
    "In this case NORMAL Chest X-Rays. For example, if the specificity is 93%, then 93% of the time we correctly predicted the X-Ray was NOT COVID-19.  This also means that 7% of the time we falsely indicated an X-Ray was not indicative of COVID-19 but in fact it was.  This scenario is a little more problematic if someone was told they were free from COVID-19, but in fact had it and inadvertantly spread the virus.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T21:53:13.483865Z",
     "start_time": "2020-03-18T21:53:13.129177Z"
    }
   },
   "outputs": [],
   "source": [
    "from build_covid_dataset import create_covid_dataset\n",
    "from sample_kaggle_dataset import create_kaggle_dataset\n",
    "dataset_root = './dataset/0318'  # because the dataset is being added to all of the time make it easy to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to github and clone the repository:\n",
    "\n",
    "https://github.com/ieee8023/covid-chestxray-dataset\n",
    "\n",
    "The function `create_covid_dataset`, will read the manifest file and pull out all of the COVID19 images and copy those to the specified output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T21:53:14.900106Z",
     "start_time": "2020-03-18T21:53:14.897490Z"
    }
   },
   "outputs": [],
   "source": [
    "covid_github_dir = '/Volumes/MacBackup/covid-chestxray-dataset'\n",
    "covid_output_dir = f'{dataset_root}/covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T21:53:16.944304Z",
     "start_time": "2020-03-18T21:53:16.849560Z"
    }
   },
   "outputs": [],
   "source": [
    "covid_file_count = create_covid_dataset(covid_github_dir, covid_output_dir)\n",
    "covid_file_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to have a Kaggle account to download the dataset.  \n",
    "\n",
    "The function `create_kaggle_dataset` will take a random sample from the dataset directory specified and put those images into the output directory.  For this experiment we are only taking images from the NORMAL training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T21:53:19.148057Z",
     "start_time": "2020-03-18T21:53:19.145311Z"
    }
   },
   "outputs": [],
   "source": [
    "kaggle_dataset_dir = '/Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/train/NORMAL'\n",
    "normal_output_dir = f'{dataset_root}/normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T21:53:21.928726Z",
     "start_time": "2020-03-18T21:53:21.849575Z"
    }
   },
   "outputs": [],
   "source": [
    "create_kaggle_dataset(kaggle_dataset_dir, normal_output_dir, covid_file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up VGG16 for transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To setup the Deep Learning models for transfer learning, we have to remove the fully connected layer.  Create the VGG16 class and specify the weights as 'imagenet' weights and include_top=False.  This will pre-initialize all of the weights to be those trained on the ImageNet dataset, and remove the top FCN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "     \t\t\t\t\t\tinput_tensor=Input(shape=(224, 224, 3)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter, `weights` is set to imagenet.  This means we want to use the kernel values for all of the convolutional matricies used to train the very large ImageNet dataset.  Doing so means we can leverage all of the training done previously on a huge dataset so we do not have to do that.\n",
    "\n",
    "The second parameter, `include_top` is set to False.  This remove the FCN layer from the VGG16 convolutional neural network.\n",
    "\n",
    "The third parameter, `input_tensor` is set to the Input shape of the images.\n",
    "\n",
    "Now that we have removed the FCN layer, we need to add our own FCN layer that wont have any weights.  The new weights for the FCN layer will need to be learned by training the model on the new Chest X-Ray data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # construct the head of the model that will be placed on top of the\n",
    "    # the base model\n",
    "    headModel = baseModel.output\n",
    "    headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(64, activation=\"relu\")(headModel)\n",
    "    headModel = Dropout(0.5)(headModel)\n",
    "    headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "    # place the head FC model on top of the base model (this will become\n",
    "    # the actual model we will train)\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start with the baseModel, which is the VGG16 model architecture, initialized with the 'imagenet' weights and we are going to add a new fully connected network (FCN) layer of 64 nodes, followed by a drop out layer randomly removing 1/2 the nodes to reduce overfitting then feed that into a 2 nodes output layer.\n",
    "\n",
    "The 2 nodes represent the probability of a COVID-19 X-Ray or a Normal X-Ray.\n",
    "\n",
    "At this point we have reconstructed the VGG16 and we are ready to retrain.\n",
    "\n",
    "\n",
    "The FCN defined above is taken directly from the blog post, but there is nothing particularly special about that configuration.  This is an area for research to determine the optimal FCN.\n",
    "\n",
    "Finally we create a model which is the combination of the baseModel which is the CNN and the new FCN model for the outputs.\n",
    "\n",
    "Recall however that when we train the model, we do <b>NOT</b> want to re-train the weights for the baseModel.  We want to use the imagenet weights.  To do that we need to freeze the baseModel.\n",
    "\n",
    "```python\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "```\n",
    "\n",
    "Keras has a number of models, and it becomes very easy for us to try many different models to see which will perform best.  \n",
    "\n",
    "To evaluate multiple models, I have refactored the original implementation to take a collection of models.\n",
    "\n",
    "```python\n",
    "        MODELS = [\n",
    "            {\n",
    "                \"base_model\": VGG16(weights=\"imagenet\", include_top=False,\n",
    "                                    input_tensor=Input(shape=(224, 224, 3))),\n",
    "                \"name\": \"vgg16\"\n",
    "            },\n",
    "            {\n",
    "                \"base_model\": VGG19(weights=\"imagenet\", include_top=False,\n",
    "                                    input_tensor=Input(shape=(224, 224, 3))),\n",
    "                \"name\": \"vgg19\"\n",
    "            },\n",
    "            {\n",
    "                \"base_model\": ResNet50(weights=\"imagenet\", include_top=False,\n",
    "                                       input_tensor=Input(shape=(224, 224, 3))),\n",
    "                \"name\": \"resnet50\"\n",
    "\n",
    "            },\n",
    "            {\n",
    "                \"base_model\": ResNet50V2(weights=\"imagenet\", include_top=False,\n",
    "                                       input_tensor=Input(shape=(224, 224, 3))),\n",
    "                \"name\": \"resnet50v2\"\n",
    "\n",
    "            }\n",
    "        ]\n",
    "\n",
    "```\n",
    "\n",
    "We are going to evaluate 4 models, using the same FCN to see how each model performs against the dataset.\n",
    "\n",
    "During testing of the models, some were not well behaved and the model was fit with a ModelCheckpoint to save the 'best' model.  The results shown below are for the 'best' model, not the predicts after the last epoch.\n",
    "\n",
    "You can find the full implementation of my version of the training script in my Github repository.\n",
    "\n",
    "For a full explaination of Adrians approach please see his [blog post](https://www.pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/) .  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we are only training the small FCN layers, it can take some time.  If you do not want to train all 4 models, then I recommend you pass a models parameter to the `train_covid_model` function with a single value.  For example:\n",
    "\n",
    "```python\n",
    "        MODELS = [\n",
    "            {\n",
    "                \"base_model\": ResNet50V2(weights=\"imagenet\", include_top=False,\n",
    "                                       input_tensor=Input(shape=(224, 224, 3))),\n",
    "                \"name\": \"resnet50v2\"\n",
    "\n",
    "            }\n",
    "        ]\n",
    "\n",
    "train_covid_model('./dataset/0318', MODELS)\n",
    "\n",
    "```\n",
    "\n",
    "To train the models, see the file `train_covid19.py`.  This can be run from the command line or imported and you can call the `train_covid_model` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:22:59.249406Z",
     "start_time": "2020-03-19T01:22:57.241618Z"
    }
   },
   "outputs": [],
   "source": [
    "from train_covid19 import train_covid_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T16:46:04.996910Z",
     "start_time": "2020-03-19T16:46:04.994389Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: calling this train method will attempt to train 4 different models on the dataset \n",
    "#       specified at the dataset_root variables.  This could take some time to run, so only uncomment \n",
    "#      if you really need to\n",
    "\n",
    "#train_covid_models(f'{dataset_root}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the training will be the following:\n",
    "\n",
    "* The best model as determine by the lowest validation loss.\n",
    "\n",
    "* The last model executed at the end of the Epochs.  Keep in mind that the best model is not always the last one trained\n",
    "\n",
    "* A chart showing loss and accuracy over the training Epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the Kaggle Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how the model works against the training/testing set is the first step but how it performs on unseen data is the real test.\n",
    "\n",
    "Unfortunately - we do not have a large collection of COVID-19 Chest X-Rays images because we had to use all of them for training.  When the Github repo can collect 100 COVID-19 images, it would be good to hold some back for final validation.\n",
    "\n",
    "Instead, what we can do is test that the model will correctly predict the normal and pneumonia X-Rays from Kaggle as NOT COVID-19.\n",
    "\n",
    "The `predict_with_model` function takes the path to a single model, and the path to the non-COVID-19 chest X-Ray images and will make predictions and show the overall results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T19:49:33.244001Z",
     "start_time": "2020-03-19T19:49:31.879449Z"
    }
   },
   "outputs": [],
   "source": [
    "from predict_covid19 import predict_with_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T19:49:33.521491Z",
     "start_time": "2020-03-19T19:49:33.519055Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = './models/best-resnet50v2-0319-model.h5'\n",
    "normal_data_path = '/Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/NORMAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T19:49:53.532496Z",
     "start_time": "2020-03-19T19:49:35.655577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COVID, NORMAL]\n",
      "[[0.5 0.5]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.7 0.3]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.4 0.6]\n",
      " [0.7 0.3]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.5 0.5]\n",
      " [0.  1. ]\n",
      " [0.9 0.1]\n",
      " [0.6 0.4]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.8 0.2]\n",
      " [0.  1. ]\n",
      " [0.9 0.1]\n",
      " [0.1 0.9]\n",
      " [0.2 0.8]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.1 0.9]\n",
      " [0.8 0.2]\n",
      " [0.1 0.9]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.2 0.8]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.4 0.6]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.6 0.4]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.7 0.3]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.4 0.6]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.4 0.6]\n",
      " [0.5 0.5]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.3 0.7]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.6 0.4]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.9 0.1]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.6 0.4]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]]\n",
      "Model: ./models/best-resnet50v2-0319-model.h5\n",
      "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/NORMAL\n",
      "Normal Accuracy: 0.8974358974358975\n"
     ]
    }
   ],
   "source": [
    "predict_with_model(model_path, normal_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "What percentage of the predictions where correct?  \n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "\n",
    "#### Recall\n",
    "\n",
    "What percentage of the positive cases (actual COVID-19) did you catch?\n",
    "\n",
    "recall = TP/(TP+FN)\n",
    "\n",
    "#### f1-score\n",
    "\n",
    "What percentage of positive (COVID-19) predictions were correct?\n",
    "\n",
    "The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0.\n",
    "\n",
    "f1-score = (2 * precision * recall)/(precision + recall)\n",
    "\n",
    "#### support\n",
    "\n",
    "Is the number of instances of each class. \n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "Matrix of Actual outcomes vs Predicted outcomes.\n",
    "\n",
    "As side note, the 0 and 1 in the confusion matrix is not an index, but the predicted value of COVID or not.\n",
    "\n",
    "```text\n",
    "                  Covid\n",
    "                  Predicted\n",
    "                 1   0 \n",
    "Covid       1 [[14  0]\n",
    "Actual      0 [ 1  13]]\n",
    "\n",
    "\n",
    "                Covid\n",
    "             Predicted\n",
    "             1   0 \n",
    "Covid    1 [[TP  FN]\n",
    "Actual   0 [ FP  TN]]\n",
    "\n",
    "```\n",
    "\n",
    "Using VGG16 numbers for COVID:\n",
    "\n",
    "- precision = TP/(TP+FP) = 14/(14+1) = 0.93\n",
    "\n",
    "- recall = TP/(TP+FN) = 14/(14+0) = 1.0 \n",
    "\n",
    "\n",
    "#### sensitivity, aka Recall or True Positive Rate ( COVID-19 Positive )\n",
    "\n",
    "sensitivity = TP/(TP + FN)\n",
    "\n",
    "How often was the model correct when the model predicted COVID-19.\n",
    "\n",
    "The percentage of positive COVID-19 cases that are correctly identified.  For example, if the sensitivity is 80%, then 20% of the time the model predicted COVID-19, when the X-Ray did not a COVID-19 patient.\n",
    "\n",
    "20% of the time the model falsely predicted COVID-19 and was actually NOT COVID-19.\n",
    "\n",
    "\n",
    "#### specificity, True Negative Rate ( COVID-19 Negative )\n",
    "\n",
    "specificity = TN/(TN+FP)\n",
    "\n",
    "Measures the proportion of actual negatives that are correctly identified.  In other words the percentage of healthy people that the model accuractely stated did not have COVID-19.  For example, if the specificity is 80%, that means 20% of the time the model was wrong to classify the X-Ray as negative, when in fact it was a positive COVID-19.  \n",
    "\n",
    "20% of the time the model falsely predicted Healthy and was actually COVID-19.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Epoch 00025: val_loss improved from 0.10472 to 0.10365, saving model to ./models/best-vgg16-0319-model.h5\n",
    "13/13 - 44s - loss: 0.1071 - accuracy: 0.9709 - val_loss: 0.1037 - val_accuracy: 0.9643\n",
    "\n",
    "[INFO] evaluating network...\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       covid       0.93      1.00      0.97        14\n",
    "      normal       1.00      0.93      0.96        14\n",
    "\n",
    "    accuracy                           0.96        28\n",
    "   macro avg       0.97      0.96      0.96        28\n",
    "weighted avg       0.97      0.96      0.96        28\n",
    "\n",
    "[[14  0]\n",
    " [ 1 13]]\n",
    "acc: 0.9643\n",
    "sensitivity: 1.0000\n",
    "specificity: 0.9286\n",
    "\n",
    "Finished Model: vgg16 took 1107.2530229091644 seconds\n",
    "\n",
    "```\n",
    "\n",
    "![vgg16](./notebook_images/vgg16-0319-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```text\n",
    "Epoch 00025: val_loss did not improve from 0.14942\n",
    "13/13 - 54s - loss: 0.1701 - accuracy: 0.9612 - val_loss: 0.2053 - val_accuracy: 0.9643\n",
    "\n",
    "[INFO] evaluating network...\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       covid       0.93      0.93      0.93        14\n",
    "      normal       0.93      0.93      0.93        14\n",
    "\n",
    "    accuracy                           0.93        28\n",
    "   macro avg       0.93      0.93      0.93        28\n",
    "weighted avg       0.93      0.93      0.93        28\n",
    "\n",
    "[[13  1]\n",
    " [ 1 13]]\n",
    "acc: 0.9286\n",
    "sensitivity: 0.9286\n",
    "specificity: 0.9286\n",
    "\n",
    "Finished Model: vgg19 took 1340.9297320842743 seconds\n",
    "\n",
    "```\n",
    "\n",
    "![vgg19](./notebook_images/vgg19-0319-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```text\n",
    "Epoch 00025: val_loss did not improve from 0.79014\n",
    "13/13 - 27s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.8588 - val_accuracy: 0.5000\n",
    "  'precision', 'predicted', average, warn_for)\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       covid       0.50      1.00      0.67        14\n",
    "      normal       0.00      0.00      0.00        14\n",
    "\n",
    "    accuracy                           0.50        28\n",
    "   macro avg       0.25      0.50      0.33        28\n",
    "weighted avg       0.25      0.50      0.33        28\n",
    "\n",
    "[[14  0]\n",
    " [14  0]]\n",
    "acc: 0.5000\n",
    "sensitivity: 1.0000\n",
    "specificity: 0.0000\n",
    "\n",
    "Finished Model: resnet50 took 677.8684051036835 seconds\n",
    "\n",
    "```\n",
    "\n",
    "![rn](./notebook_images/resnet50-0319-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```text\n",
    "Epoch 00025: val_loss did not improve from 0.07534\n",
    "13/13 - 23s - loss: 0.1394 - accuracy: 0.9417 - val_loss: 0.7662 - val_accuracy: 0.8214\n",
    "[INFO] evaluating network...\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       covid       1.00      0.93      0.96        14\n",
    "      normal       0.93      1.00      0.97        14\n",
    "\n",
    "    accuracy                           0.96        28\n",
    "   macro avg       0.97      0.96      0.96        28\n",
    "weighted avg       0.97      0.96      0.96        28\n",
    "\n",
    "[[13  1]\n",
    " [ 0 14]]\n",
    "acc: 0.9643\n",
    "sensitivity: 0.9286\n",
    "specificity: 1.0000\n",
    "\n",
    "Finished Model: resnet50v2 took 584.7564489841461 seconds\n",
    "```\n",
    "\n",
    "![rnv2](./notebook_images/resnet50v2-0319-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T22:57:24.536491Z",
     "start_time": "2020-03-19T22:57:24.529419Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.DataFrame(data=[[0.9642857142857143, 1.0, 0.9285714285714286, 'vgg16'],\n",
    "                    [0.9285714285714286, 0.9285714285714286, 0.9285714285714286, 'vgg19'],\n",
    "                    [0.5, 1.0, 0.0, 'resnet50'],\n",
    "                    [0.9642857142857143, 0.9285714285714286, 1.0, 'resnet50v2']],\n",
    " columns=[\"accuracy\", \"sensitivity\",\"specificity\",\"model name\"]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T22:57:32.645547Z",
     "start_time": "2020-03-19T22:57:32.625427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>model name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>vgg16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>vgg19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>resnet50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>resnet50v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  sensitivity  specificity  model name\n",
       "0  0.964286     1.000000     0.928571       vgg16\n",
       "1  0.928571     0.928571     0.928571       vgg19\n",
       "2  0.500000     1.000000     0.000000    resnet50\n",
       "3  0.964286     0.928571     1.000000  resnet50v2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the results it is clear we cn remove `resnet50` but `vgg16` and `resnet50v2` are good candidates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the Kaggle Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how the model works against the training/testing set is the first step but how it performs on unseen data is the real test.\n",
    "\n",
    "Unfortunately - we do not have a large collection of COVID-19 Chest X-Rays images because we had to use all of them for training.  When the Github repo can collect 100 COVID-19 images, it would be good to hold some back for final validation.\n",
    "\n",
    "Instead, what we can do is test that the model will correctly predict the normal and pneumonia X-Rays from Kaggle as NOT COVID-19.\n",
    "\n",
    "The `predict_with_model` function takes the path to a single model, and the path to the non-COVID-19 chest X-Ray images and will make predictions and show the overall results.\n",
    "\n",
    "Below is an example of running the best `resnet50v2` model against the NORMAL Kaggle Chest X-Rays.  You can do this for each of the models and both for NORMAL AND PNEUMONIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T23:01:10.302983Z",
     "start_time": "2020-03-19T23:01:10.298221Z"
    }
   },
   "outputs": [],
   "source": [
    "from predict_covid19 import predict_with_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T23:01:12.180993Z",
     "start_time": "2020-03-19T23:01:12.178360Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = './models/best-resnet50v2-0319-model.h5'\n",
    "normal_data_path = '/Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/NORMAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T23:01:46.487798Z",
     "start_time": "2020-03-19T23:01:28.554342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COVID, NORMAL]\n",
      "[[0.7 0.3]\n",
      " [0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.8 0.2]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.7 0.3]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.7 0.3]\n",
      " [0.  1. ]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.1 0.9]\n",
      " [0.3 0.7]\n",
      " [0.9 0.1]\n",
      " [0.  1. ]\n",
      " [0.8 0.2]\n",
      " [0.3 0.7]\n",
      " [0.3 0.7]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.1 0.9]\n",
      " [0.9 0.1]\n",
      " [0.1 0.9]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.4 0.6]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.3 0.7]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.3 0.7]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.6 0.4]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.6 0.4]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.7 0.3]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.6 0.4]\n",
      " [0.3 0.7]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.5 0.5]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.3 0.7]\n",
      " [0.  1. ]\n",
      " [0.5 0.5]\n",
      " [0.8 0.2]\n",
      " [0.6 0.4]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.4 0.6]\n",
      " [0.  1. ]\n",
      " [0.7 0.3]\n",
      " [0.5 0.5]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.5 0.5]\n",
      " [0.8 0.2]\n",
      " [0.  1. ]\n",
      " [0.1 0.9]\n",
      " [0.1 0.9]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.6 0.4]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.7 0.3]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.9 0.1]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.7 0.3]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]]\n",
      "Model: ./models/best-resnet50v2-0319-model.h5\n",
      "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/NORMAL\n",
      "Normal Accuracy: 0.8547008547008547\n"
     ]
    }
   ],
   "source": [
    "predict_with_model(model_path, normal_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T23:03:52.277583Z",
     "start_time": "2020-03-19T23:03:52.266772Z"
    }
   },
   "source": [
    "```text\n",
    "PREDICT NORMAL\n",
    "\n",
    "Model: ./models/best-vgg16-0319-model.h5\n",
    "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/NORMAL\n",
    "Normal Accuracy: 0.7649572649572649\n",
    "\n",
    "Model: ./models/best-vgg19-0319-model.h5\n",
    "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/NORMAL\n",
    "Normal Accuracy: 0.7991452991452992\n",
    "\n",
    "Model: ./models/best-resnet50-0319-model.h5\n",
    "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/NORMAL\n",
    "Normal Accuracy: 0.0\n",
    "\n",
    "Model: ./models/best-resnet50v2-0319-model.h5\n",
    "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/NORMAL\n",
    "Normal Accuracy: 0.8974358974358975\n",
    "\n",
    "PREDICT PNEUMONIA\n",
    "\n",
    "Model: ./models/best-vgg16-0319-model.h5\n",
    "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/PNEUMONIA\n",
    "PNEUMONIA Accuracy: 0.5615384615384615\n",
    "\n",
    "Model: ./models/best-vgg19-0319-model.h5\n",
    "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/PNEUMONIA\n",
    "PNEUMONIA Accuracy: 0.382051282051282\n",
    "\n",
    "Model: ./models/best-resnet50-0319-model.h5\n",
    "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/PNEUMONIA\n",
    "PNEUMONIA Accuracy: 0.0\n",
    "\n",
    "Model: ./models/best-resnet50v2-0319-model.h5\n",
    "Dataset: /Volumes/MacBackup/kaggle-chest-x-ray-images/chest_xray/test/PNEUMONIA\n",
    "PNEUMONIA Accuracy: 0.9179487179487179\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, you can see that the best overall model was the `resnet50v2`.  \n",
    "\n",
    "This seems a little surprising just using the Train/Test Validation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
